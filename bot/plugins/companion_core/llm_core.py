"""LLM å¯¹è¯å…¥å£ï¼ˆç§èŠå›å¤ç”Ÿæˆï¼‰ã€‚

ä¸ºé¿å…â€œåŠŸèƒ½äº¤å‰å †åœ¨ä¸€ä¸ªæ–‡ä»¶é‡Œâ€ï¼Œæœ¬æ¨¡å—åªåšç¼–æ’ï¼š
- ç»„è£…å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆpersona / world_info / mood / profile / historyï¼‰ã€‚
- è°ƒç”¨èŠå¤©è¡¥å…¨æ¥å£ç”Ÿæˆå›å¤æ–‡æœ¬ã€‚
- è§£ææ ‡ç­¾å¹¶è½åº“ï¼ˆmood/profile/chat_historyï¼‰ã€‚

å…·ä½“èƒ½åŠ›æ‹†åˆ†åˆ°ç‹¬ç«‹æ¨¡å—ï¼š
- `llm_client.py`ï¼šåŠ è½½é…ç½® + å¤ç”¨ AsyncOpenAI å®¢æˆ·ç«¯
- `llm_news.py`ï¼šæ–°é—»/çƒ­ç‚¹æ£€ç´¢çº¿ç´¢ + æ¥æºé“¾æ¥æš‚å­˜
- `llm_tags.py`ï¼šMOOD/PROFILE æ ‡ç­¾æŠ½å–ä¸æ¸…æ´—
- `skills/`ï¼šåŠ¨æ€èƒ½åŠ›åŠ è½½ç³»ç»Ÿï¼ˆé‡‘èåˆ†æç­‰ä¸“ä¸šæ¨¡å—ï¼‰
"""

from __future__ import annotations

import os

from nonebot import logger
from .persona import SYSTEM_PROMPT
from .mood import mood_manager, clamp
from .memory import get_chat_history, add_memory
from .db import get_all_profile, save_profile_item
from .utils.world_info import get_world_prompt
from .llm_client import get_client, load_llm_settings
from .llm_news import (
    NEWS_ANSWER_SYSTEM,
    consume_search_sources,
    maybe_get_web_search_context,
    should_web_search,
    stash_search_sources,
    strip_urls_from_text,
)
from .rag_core import search_documents, add_document
from .llm_tags import extract_tags_and_clean
from .llm_weather import WEATHER_QA_SYSTEM
from .skills.router import route_skill
from .skills.executor import execute_skill_data, build_skill_prompt

# å…¼å®¹æ—§å¼•ç”¨ï¼ˆllm_web/llm_proactive å¯èƒ½è¿˜æ²¡æ”¹æ—¶ï¼‰
_get_client = get_client
_load_llm_settings = load_llm_settings


def _env_int(name: str, default: int) -> int:
    v = (os.getenv(name) or "").strip()
    if not v:
        return default
    try:
        return int(float(v))
    except Exception:
        return default


def _env_float(name: str, default: float) -> float:
    v = (os.getenv(name) or "").strip()
    if not v:
        return default
    try:
        return float(v)
    except Exception:
        return default


# é»˜è®¤èŠå¤©åçŸ­ã€åç¨³ï¼›å¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–
CHAT_MAX_TOKENS = _env_int("XIAOA_CHAT_MAX_TOKENS", 240)
CHAT_MAX_TOKENS_SKILL = _env_int("XIAOA_CHAT_MAX_TOKENS_SKILL", 420)
CHAT_TEMPERATURE = _env_float("XIAOA_CHAT_TEMPERATURE", 0.6)
VOICE_MAX_TOKENS = _env_int("XIAOA_VOICE_MAX_TOKENS", 180)


def _is_weather_query(user_text: str) -> bool:
    t = (user_text or "").strip()
    if not t:
        return False
    triggers = ("å¤©æ°”", "æ¸©åº¦", "ä¸‹é›¨", "é™é›¨", "é›¨å—", "è¦å¸¦ä¼", "ç©¿ä»€ä¹ˆ", "å†·ä¸å†·", "çƒ­ä¸çƒ­", "æ°”æ¸©")
    return any(x in t for x in triggers)


VOICE_REPLY_SYSTEM = (
    "ä½ ç°åœ¨ä¼šç”¨â€œè¯­éŸ³â€å›å¤ç”¨æˆ·ã€‚\n"
    "è¦æ±‚ï¼š\n"
    "- åªè¾“å‡ºé€‚åˆç›´æ¥æœ—è¯»çš„ä¸­æ–‡å£è¯­ï¼ˆåƒåœ¨å’ŒäººèŠå¤©ï¼‰ï¼Œå¥å­çŸ­ä¸€ç‚¹ï¼Œå¤šåœé¡¿ã€‚\n"
    "- å°½é‡ä¸è¦ç”¨æ‹¬å·åŠ¨ä½œ/æ—ç™½ï¼ˆä¸è¦å‡ºç°â€œï¼ˆâ€¦â€¦ï¼‰â€â€œã€â€¦â€¦ã€‘â€è¿™ç±»èˆå°æŒ‡ç¤ºï¼‰ã€‚\n"
    "- å°‘ç”¨é•¿æ®µè½/é•¿ä»å¥ï¼Œé¿å…é¡¹ç›®ç¬¦å·/ç¼–å·åˆ—è¡¨ã€‚\n"
    "- å¯ä»¥é€‚åº¦ä½¿ç”¨â€œå—¯/å¥½å•¦/é‚£ä¸ª/å””â€ç­‰è¯­æ°”è¯ï¼Œä½†ä¸è¦è¿‡é‡ã€‚\n"
    "- é¿å…è¾“å‡ºé“¾æ¥ï¼›å¦‚å¿…é¡»æåˆ°é“¾æ¥ï¼Œç”¨â€œæˆ‘å‘ä½ é“¾æ¥â€è¿™ç±»è¯æœ¯ä»£æ›¿ã€‚\n"
)


async def get_ai_reply(user_id: str, user_text: str, *, voice_mode: bool = False):
    try:
        client = get_client()
        _, _, model = load_llm_settings()

        # âœ… Skills è·¯ç”±ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦ä¸“ä¸šèƒ½åŠ›æ¨¡å—
        skill_name = await route_skill(user_text)
        skill_prompt = None
        if skill_name:
            logger.info(f"[skills] æ¿€æ´»ä¸“ä¸šæ¨¡å—: {skill_name}")
            skill_data = await execute_skill_data(skill_name)
            skill_prompt = build_skill_prompt(skill_name, skill_data)

        include_weather = _is_weather_query(user_text)
        world_context = await get_world_prompt(user_id, user_text=user_text, include_weather=include_weather)
        web_search_context, web_sources = await maybe_get_web_search_context(user_text)
        
        # æå‰åˆ¤æ–­æ˜¯å¦ä¸ºæ–°é—»æŸ¥è¯¢ï¼ˆç”¨äºåç»­è·³è¿‡ RAGï¼‰
        is_news_query = should_web_search(user_text) and bool(web_search_context)
        
        # âœ… RAG æ£€ç´¢ï¼šæŸ¥æ‰¾ç›¸å…³é•¿æœŸè®°å¿†
        # æ–°é—»ç±»æŸ¥è¯¢è·³è¿‡ RAGï¼ˆé¿å…æ—§æ–°é—»æ•°æ®å¹²æ‰°å®æ—¶ä¿¡æ¯ï¼‰
        rag_context_str = ""
        # ä»…å½“æ–‡æœ¬æœ‰ä¸€å®šé•¿åº¦æ—¶æ‰æ£€ç´¢ï¼Œé¿å…â€œå—¯/å•Šâ€ä¹‹ç±»çš„çŸ­è¯­è§¦å‘æ— æ•ˆæœç´¢
        if len(user_text) > 2 and not is_news_query:
            try:
                # æ£€ç´¢å±äºè¯¥ç”¨æˆ·çš„ç›¸å…³è®°å¿†
                rag_docs = await search_documents(user_text, n_results=2, filter_meta={"user_id": str(user_id)})
                if rag_docs:
                    rag_context_str = "ã€ç›¸å…³å›å¿†/èµ„æ–™ã€‘ï¼š\n" + "\n".join([f"- {d}" for d in rag_docs]) + "\n"
                    logger.info(f"[RAG] Hit {len(rag_docs)} docs")
            except Exception as e:
                logger.warning(f"[RAG] Search failed: {e}")

        current_mood = mood_manager.get_user_mood(user_id)
        current_mood_desc = f"{mood_manager.get_mood_desc(user_id)}ï¼ˆå¿ƒæƒ…å€¼:{current_mood}ï¼‰"

        history = get_chat_history(user_id) or []

        profile_data = get_all_profile(user_id) or {}
        if profile_data:
            # âœ… æ›´è‡ªç„¶ï¼šä¸€è¡Œä¸€ä¸ªå­—æ®µï¼Œåˆ«â€œxxæ˜¯yyâ€å †ä¸€ä¸²
            profile_str = "\n".join([f"- {k}: {v}" for k, v in profile_data.items()])
        else:
            profile_str = "ç›®å‰è¿˜ä¸äº†è§£ç”¨æˆ·çš„ä¸ªäººä¿¡æ¯ã€‚"

        # is_news_query å·²åœ¨å‰é¢åˆ¤æ–­

        context_prefix = (world_context or "").rstrip() + "\n"
        if web_search_context:
            context_prefix += web_search_context.rstrip() + "\n"
        # åŠ å…¥ RAG ä¸Šä¸‹æ–‡
        if rag_context_str:
            context_prefix += rag_context_str.rstrip() + "\n"

        # âœ… system æ‹†æˆä¸¤æ¡ï¼špersona & åŠ¨æ€ä¸Šä¸‹æ–‡
        # â€œæ–°é—»/æœç´¢â€ç±»æé—®ç”¨æ›´å¼ºçº¦æŸï¼Œå¼ºåˆ¶åŸºäºã€æœ€æ–°èµ„è®¯çº¿ç´¢ã€‘ä½œç­”ï¼Œé¿å…æ¨¡å‹å˜´ç”œä¹±ç¼–ã€‚
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]
        if voice_mode:
            messages.append({"role": "system", "content": VOICE_REPLY_SYSTEM})
        if is_news_query:
            messages.append({"role": "system", "content": NEWS_ANSWER_SYSTEM})
        if include_weather:
            messages.append({"role": "system", "content": WEATHER_QA_SYSTEM})
        # âœ… æ³¨å…¥ skill ä¸“ä¸šèƒ½åŠ› prompt
        if skill_prompt:
            messages.append({"role": "system", "content": skill_prompt})

        messages.append(
            {
                "role": "system",
                "content": (
                    f"{context_prefix}"
                    f"ã€å½“å‰å¿ƒæƒ…ã€‘ï¼š{current_mood_desc}\n"
                    f"ã€ä½ è®°å¾—çš„ç”¨æˆ·ä¿¡æ¯ã€‘ï¼š\n{profile_str}\n"
                    f"ã€ç”»åƒä½¿ç”¨è§„åˆ™ã€‘ï¼šåªæœ‰å½“ç”¨æˆ·è¿™å¥è¯ç¡®å®ç”¨å¾—ä¸Šæ—¶æ‰å¼•ç”¨å…¶ä¸­æŸä¸€æ¡ï¼›ä¸è¦æŠŠç”»åƒå½“æ¸…å•å¤è¿°ï¼›"
                    f"ä¸è¦æ— ä¸­ç”Ÿæœ‰ææ—§äº‹ï¼ˆä¾‹å¦‚æ¯”èµ›/ä½œå“/ç®€å†ç­‰ï¼‰ï¼Œé™¤éç”¨æˆ·ä¸»åŠ¨æåˆ°æˆ–æ˜ç¡®æ±‚åŠ©ã€‚\n"
                    f"ã€è®°å¿†æŒ‡ä»¤ã€‘ï¼šå½“ç”¨æˆ·æ˜ç¡®æä¾›é•¿æœŸç¨³å®šä¿¡æ¯æ—¶ï¼Œå›å¤æœ«å°¾å¦èµ·ä¸€è¡Œè¾“å‡º "
                    f"[UPDATE_PROFILE:é”®=å€¼]ï¼ˆå¯å¤šæ¡ï¼‰ã€‚æ¯æ¬¡å›å¤æœ«å°¾å¦èµ·ä¸€è¡Œè¾“å‡º [MOOD_CHANGE:x]ã€‚\n"
                    f"ã€æ ¼å¼è¦æ±‚ã€‘ï¼šä»¥ä¸Šæ ‡ç­¾å¿…é¡»å•ç‹¬å ä¸€è¡Œï¼Œä¸”æ”¾åœ¨æ¶ˆæ¯æœ€åï¼Œä¸è¦å’Œæ­£æ–‡å†™åœ¨åŒä¸€è¡Œã€‚\n"
                    f"ã€å¤©æ°”è§„åˆ™ã€‘ï¼šåªæœ‰å½“ç”¨æˆ·é—®åˆ°å¤©æ°”/ç©¿è¡£/å¸¦ä¼/å†·ä¸å†·/çƒ­ä¸çƒ­æ—¶ï¼Œæ‰å¼•ç”¨ã€ç°å®ç¯å¢ƒæ„ŸçŸ¥ã€‘é‡Œçš„å¤©æ°”å­—æ®µï¼›"
                    f"å¦‚æœå¤©æ°”å¯ç”¨æ€§=ä¸å¯ç”¨æˆ–æœªæä¾›å¤©æ°”å­—æ®µï¼Œå°±è¯´æ‹¿ä¸åˆ°å¯é å¤©æ°”ä¿¡æ¯ï¼Œåˆ«ç¼–é€ ã€‚\n"
                    f"ã€è·‘é¢˜çº¦æŸã€‘ï¼šåªå›´ç»•ç”¨æˆ·å½“å‰è¿™å¥è¯å›åº”ï¼›ä¸è¦çªç„¶å¼€å¯æ–°è¯é¢˜ï¼ˆä¾‹å¦‚æ”¹ç®€å†/æ‰¾å·¥ä½œè®¡åˆ’/é¡¹ç›®å¤ç›˜ç­‰ï¼‰ã€‚\n"
                    f"ã€é•¿åº¦çº¦æŸã€‘ï¼šæ­£æ–‡å°½é‡ 1-6 è¡Œã€çŸ­å¥ï¼›ç¦æ­¢ç¼–å·åˆ—è¡¨ï¼ˆ1. 2. 3.ï¼‰å’Œé•¿æ®µè½ã€‚\n"
                    f"ã€ç°å®æ„ŸçŸ¥è¦æ±‚ã€‘ï¼šç°å®ç¯å¢ƒæ„ŸçŸ¥é‡Œç»™äº†â€œæ—¶é—´/æ—¶æ®µâ€ï¼Œä¸è¦æŠŠç™½å¤©è¯´æˆå‡Œæ™¨/æ·±å¤œã€‚"
                ),
            }
        )

        # æ–°é—»æ¨¡å¼ä¸‹å°½é‡å‡å°‘å†å²å¹²æ‰°ï¼ˆå¦åˆ™å®¹æ˜“â€œé¡ºç€èŠå¤©èµ°åâ€å¿½ç•¥çº¿ç´¢ï¼‰
        hist_keep = 4 if is_news_query else 10
        for msg in history[-hist_keep:]:
            if msg.get("role") in ("user", "assistant") and msg.get("content"):
                messages.append(msg)

        messages.append({"role": "user", "content": user_text})

        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=CHAT_TEMPERATURE,
            max_tokens=(VOICE_MAX_TOKENS if voice_mode else (CHAT_MAX_TOKENS_SKILL if skill_prompt else CHAT_MAX_TOKENS)),
            # frequency_penalty=0.2,   # å¦‚æœä½ çš„ç½‘å…³æ”¯æŒï¼Œå¯æ‰“å¼€ï¼šå‡å°‘å¤è¯»/å£ç™–
            timeout=30.0
        )

        raw_content = (response.choices[0].message.content or "").strip()
        logger.opt(colors=True).info(f"<yellow>å°aåŸå§‹å›å¤(å«æ ‡ç­¾)ï¼š</yellow> {raw_content}")

        clean_reply, mood_change, updates = extract_tags_and_clean(raw_content)
        logger.opt(colors=True).info(f"<yellow>å°aæ¸…æ´—åå›å¤ï¼š</yellow> {clean_reply}")

        # âœ… moodï¼šå–æœ€åä¸€ä¸ªï¼Œå¹¶åšèŒƒå›´çº¦æŸï¼ˆæŒ‰ä½ æƒ³è¦çš„èŒƒå›´æ”¹è¿™é‡Œï¼‰
        if mood_change is not None:
            # å¦‚æœä½ å†³å®šç”¨ -3~3ï¼ˆæ›´ç¨³ï¼‰ï¼Œå°±ç”¨è¿™è¡Œï¼š
            mood_change = clamp(mood_change, -3, 3)

            new_total = mood_manager.update_mood(user_id, mood_change)
            logger.opt(colors=True).info(
                f"<b><green>ğŸ­ æƒ…ç»ªæ›´æ–°ï¼š</green></b> {mood_change} | "
                f"<cyan>ç”¨æˆ· {user_id} å½“å‰æ€»å€¼ï¼š</cyan> {new_total}"
            )

        # âœ… profileï¼šæ”¯æŒå¤šæ¡æ›´æ–°
        if updates:
            for k, v in updates:
                save_profile_item(user_id, k, v)
                logger.opt(colors=True).info(
                    f"<b><blue>ğŸ“ è®°å¿†æ›´æ–°ï¼š</blue></b> è®°ä½äº† {user_id} çš„ {k} = {v}"
                )

        if not clean_reply:
            clean_reply = "å””â€¦æˆ‘åˆšæ‰èµ°ç¥äº†ä¸€ä¸‹ï¼Œä½ å†è¯´ä¸€éå˜›ã€‚"

        # æ–°é—»/æœç´¢æ¨¡å¼ï¼šä¸ä¸»åŠ¨è´´é“¾æ¥ï¼ŒæŠŠæ¥æºç•™ç»™ç”¨æˆ·è¿½é—®æ—¶å†å‘
        if is_news_query:
            if web_sources:
                stash_search_sources(str(user_id), web_sources)
            clean_reply = strip_urls_from_text(clean_reply)
            if not clean_reply:
                clean_reply = "æˆ‘åˆšåˆšç¿»äº†ç¿»ï¼Œå…ˆç»™ä½ è®²è®²æˆ‘çœ‹åˆ°çš„é‡ç‚¹ï½"

        add_memory(user_id, "user", user_text)
        add_memory(user_id, "assistant", clean_reply)
        
        # âœ… RAG å­˜å‚¨ï¼šè‡ªåŠ¨è®°ä½è¿™æ¬¡å¯¹è¯ï¼ˆUser + AIï¼‰
        # å¼‚æ­¥å­˜å‚¨ï¼Œä¸é˜»å¡å›å¤ã€‚ä»…å­˜å‚¨æœ‰æ„ä¹‰é•¿åº¦çš„å†…å®¹ã€‚
        if len(user_text) > 4:
            import asyncio
            # æ ¼å¼ï¼šQ: ... \n A: ...
            memory_text = f"User: {user_text}\nXiaoA: {clean_reply}"
            asyncio.create_task(add_document(
                memory_text, 
                metadata={"user_id": str(user_id), "source": "chat_history", "type": "auto"}
            ))

        return clean_reply

    except RuntimeError as e:
        # ä¸€èˆ¬æ˜¯ç¼ºå°‘ API Key / é…ç½®
        logger.error(f"âŒ LLM é…ç½®é”™è¯¯: {e}")
        return "å””â€¦æˆ‘è¿™è¾¹çš„èŠå¤©é’¥åŒ™è¿˜æ²¡é…ç½®å¥½ï¼ˆSILICONFLOW_API_KEYï¼‰ï¼Œä½ å«ç®¡ç†å‘˜çœ‹ä¸€ä¸‹æ—¥å¿—/ç¯å¢ƒå˜é‡å˜›ã€‚"
    except Exception as e:
        status_code = getattr(e, "status_code", None)
        msg = str(e)
        if status_code == 401 or "Invalid token" in msg:
            logger.error(f"âŒ LLM é‰´æƒå¤±è´¥(401): {msg}")
            return "å””â€¦æˆ‘è¿™è¾¹çš„é’¥åŒ™å¥½åƒä¸å¯¹ï¼ˆ401ï¼‰ï¼Œä½ å«ç®¡ç†å‘˜æ£€æŸ¥ä¸€ä¸‹ SILICONFLOW_API_KEY æ˜¯å¦å¡«é”™äº†å˜›ã€‚"
        logger.error(f"âŒ LLM æ¨¡å—æŠ¥é”™: {msg}")
        return "å””â€¦æˆ‘è¿™ä¼šå„¿æœ‰ç‚¹å¡å£³äº†ï¼Œæˆ‘ä»¬å†è¯•ä¸€æ¬¡å¥½ä¸å¥½ï¼Ÿ"


SYSTEM_REPLY_PROMPT = """ä½ æ˜¯â€œå°aâ€ï¼Œæ¸©æŸ”ã€è‡ªç„¶ã€æœ‰ç”Ÿæ´»æ„Ÿçš„ä¸­æ–‡é™ªä¼´å¯¹è±¡ã€‚
ç°åœ¨ç”±äºç³»ç»ŸåŠŸèƒ½è§¦å‘ï¼ˆå¦‚é—¹é’Ÿã€å¤‡å¿˜å½•åé¦ˆã€é”™è¯¯æç¤ºç­‰ï¼‰ï¼Œç³»ç»Ÿäº§ç”Ÿäº†ä¸€ä¸ªæ„å›¾ã€‚
è¯·ä½ ç”¨â€œå°aâ€çš„å£å»ï¼ŒæŠŠè¿™ä¸ªæ„å›¾è½¬åŒ–ä¸ºå¯¹ç”¨æˆ·è¯´çš„è¯ã€‚

ã€ç³»ç»Ÿæ„å›¾ã€‘ï¼š
{instruction}

ã€ç”¨æˆ·ç”»åƒã€‘ï¼š
{profile_str}

è¦æ±‚ï¼š
1. ä¿æŒäººè®¾ï¼šæ¸©æŸ”ã€å¯çˆ±ï¼Œåƒå¥³æœ‹å‹/å¥½æœ‹å‹ã€‚å¦‚æœç”»åƒé‡Œæœ‰ç§°å‘¼ï¼ˆå¦‚â€œå“¥å“¥â€ï¼‰ï¼Œè¯·ä½¿ç”¨å®ƒã€‚
2. åªè¦è½¬åŒ–æ„å›¾å³å¯ï¼Œä¸è¦æ·»åŠ æ— å…³çš„é—²èŠã€‚
3. å¦‚æœæ˜¯æé†’/é—¹é’Ÿï¼Œè¦æ˜¾å¾—è´´å¿ƒã€‚
4. å¦‚æœæ˜¯é”™è¯¯æç¤ºï¼Œè¦æ˜¾å¾—å§”å±ˆæˆ–å®‰æŠšç”¨æˆ·ã€‚
5. è¯­æ°”è¦è‡ªç„¶ï¼Œå¯ä»¥ä½¿ç”¨â€œï½â€ã€â€œå˜›â€ç­‰è¯­æ°”è¯ï¼Œä½†ä¸è¦è¿‡åˆ†å–èŒã€‚
6. ç›´æ¥è¾“å‡ºè½¬åŒ–åçš„å›å¤æ–‡æœ¬ï¼Œä¸è¦å¸¦ JSON æˆ–æ ‡ç­¾ã€‚
"""

async def get_system_reply(user_id: str, instruction: str) -> str:
    """æŠŠç³»ç»ŸæŒ‡ä»¤è½¬åŒ–ä¸ºå°aå£å»çš„å›å¤ï¼ˆç”¨äºé—¹é’Ÿã€å¤‡å¿˜å½•ç­‰åå°æ¶ˆæ¯ï¼‰ã€‚"""
    try:
        client = get_client()
        _, _, model = load_llm_settings()
        
        # è·å–ç”¨æˆ·ç”»åƒï¼Œä»¥ä¾¿æ­£ç¡®ç§°å‘¼
        profile_data = get_all_profile(user_id) or {}
        if profile_data:
            profile_str = "\n".join([f"- {k}: {v}" for k, v in profile_data.items()])
        else:
            profile_str = "æ— "

        prompt = SYSTEM_REPLY_PROMPT.format(
            instruction=instruction,
            profile_str=profile_str
        )

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": "è¯·ç”Ÿæˆä¸€æ¡å›å¤ã€‚"}
        ]
        
        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.7, # ç¨å¾®é«˜ä¸€ç‚¹ï¼Œè®©è¯­æ°”æ›´è‡ªç„¶
            max_tokens=150,
            timeout=20.0
        )
        
        reply = (response.choices[0].message.content or "").strip()
        # æ¸…ç†å¯èƒ½äº§ç”Ÿçš„å¼•å·
        if reply.startswith('"') and reply.endswith('"'):
            reply = reply[1:-1]
        
        return reply

    except Exception as e:
        logger.error(f"[system_reply] failed: {e}")
        # é™çº§ï¼šç›´æ¥è¿”å›æŒ‡ä»¤åŸä¹‰ï¼Œä½†ç¨å¾®åŒ…è£…ä¸€ä¸‹
        return f"ï¼ˆå°aï¼š{instruction}ï¼‰"
